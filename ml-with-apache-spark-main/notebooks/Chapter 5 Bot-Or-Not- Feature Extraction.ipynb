{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Bot-Or-Not feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
    "from pyspark.ml.fpm import FPGrowth, FPGrowthModel\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"Pipelines\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.parquet(\"classified_train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.fillna({'bot':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('train_data_only_description')\n",
    "data = data.fillna({'label':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"description\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Extraction\n",
    "## TF-IDF feature extraction\n",
    "Leveraging text technique to try and extract meaningful features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    1|\n",
      "|   13|\n",
      "|    3|\n",
      "|   19|\n",
      "|   10|\n",
      "|    0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsData.select('label').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|   frequencyFeatures|\n",
      "+-----+--------------------+\n",
      "|    1|(20,[0,2,3,4,5,7,...|\n",
      "|    0|(20,[3,13,16,17],...|\n",
      "|    0|(20,[1,2,4,5,6,7,...|\n",
      "|    0|(20,[0,1,4,5,7,8,...|\n",
      "|    1|(20,[0,1,3,4,5,6,...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first TF using HashingTF. alternatively, we can use CountVectorizer to get term frequency vectors\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"frequencyFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "featurizedData.select(\"label\", \"frequencyFeatures\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    1|(20,[0,2,3,4,5,7,...|\n",
      "|    0|(20,[3,13,16,17],...|\n",
      "|    0|(20,[1,2,4,5,6,7,...|\n",
      "|    0|(20,[0,1,4,5,7,8,...|\n",
      "|    1|(20,[0,1,3,4,5,6,...|\n",
      "|    0|(20,[5,11,15,16],...|\n",
      "|    0|(20,[5,6,8,10,12,...|\n",
      "|    0|(20,[2,6,8,12,16,...|\n",
      "|    0|(20,[1,3,7,8,9,10...|\n",
      "|   13|(20,[1,3,4,5,7,8,...|\n",
      "|    1|(20,[1,4,5,7,8,9,...|\n",
      "|    0|(20,[0,1,2,8,10,1...|\n",
      "|    0|(20,[0,1,2,5,7,11...|\n",
      "|    0|(20,[2,5,6,7,8,9,...|\n",
      "|    0|(20,[0,4,5,9,11,1...|\n",
      "|    0|(20,[3,10,15,16],...|\n",
      "|    0|(20,[0,1,3,4,6,7,...|\n",
      "|    0|(20,[0,1,3,4,5,8,...|\n",
      "|    0|(20,[0,2,4,5,7,11...|\n",
      "|    0|(20,[0,1,3,4,17,1...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# second IDF model\n",
    "\n",
    "idf = IDF(inputCol=\"frequencyFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ngrams                                                                                                                                                                                                                                                                                                                                           |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[contributing editor, editor for, for network, network ten, ten based, based in, in canberra., canberra. his, his views, views contestable, contestable but, but his, his own.]                                                                                                                                                                  |\n",
      "|[i live, live in, in texas]                                                                                                                                                                                                                                                                                                                      |\n",
      "|[fresh e3, e3 rumours, rumours every, every 3, 3 hours!, hours! made, made by, by @warandpeace, @warandpeace with, with http://t.co/xpduja7q2i., http://t.co/xpduja7q2i. inspired, inspired by, by @theblacknerd,, @theblacknerd, @sokareemie,, @sokareemie, @mikesacco,, @mikesacco, &, & @tinysubversions]                                     |\n",
      "|[''the 'hello, 'hello world', world' of, of jokey,, jokey, generative, generative twitter, twitter bots.'', bots.'' -, - @joshmillard, @joshmillard creates, creates bad, bad ideas, ideas for, for startups., startups. made, made by, by @norareed, @norareed with, with help, help from, from just_post.]                                     |\n",
      "|[proud west, west belconnian,, belconnian, local, local labor, labor member, member ,  4, 4 ginninderra,, ginninderra, minister, minister in, in the, the act, act govt,, govt, mum, mum of, of 2, 2 with, with cassie, cassie cupcake, cupcake k9, k9 campaigner,, campaigner, chook, chook lady,, lady, love, love everything, everything cbr.]|\n",
      "|[hello, i, i am, am here, here https://t.co/et03dwzhh6]                                                                                                                                                                                                                                                                                          |\n",
      "|[meow! i, i want, want to, to talk, talk here-, here- https://t.co/2vhss3j9ef]                                                                                                                                                                                                                                                                   |\n",
      "|[i have, have something, something to, to say, say about, about all, all the, the games.]                                                                                                                                                                                                                                                        |\n",
      "|[i have, have more, more than, than a, a thousand, thousand valuable, valuable alots,, alots, known, known by, by their, their countless, countless abilities., abilities. i, i created, created this, this account, account to, to help, help those, those people, people in, in need, need of, of an, an alot.]                                |\n",
      "|[if you, you have, have to, to start, start a, a sentence, sentence with, with 'i'm, 'i'm not, not racist,, racist, but...', but...' then, then chances, chances are, are you're, you're pretty, pretty racist., racist. this, this isn't, isn't a, a bot., bot. rtš_¾endorsement,, rtš_¾endorsement, obviously.]                                |\n",
      "|[i am, am a, a twitterbot, twitterbot posting, posting #transposable, #transposable element, element papers, papers from, from #pubmed, #pubmed and, and #biorxiv., #biorxiv. curated, curated by, by @nels42]                                                                                                                                   |\n",
      "|[designing and, and making, making games, games @nytdesign, @nytdesign _ò, _ò side, side project, project haver, haver _ò, _ò see, see also:, also: @itsthisyear,, @itsthisyear, @icebergdotcool,, @icebergdotcool, @ramsophone,, @ramsophone, @__birds__, @__birds__ _ò]                                                                   |\n",
      "|[host of, of vleeties, vleeties live, live on, on wednesdays, wednesdays at, at 11, 11 est., est. co, co host, host of, of twfs!, twfs! new, new videos, videos weekly., weekly. wrestling, wrestling fan!, fan! married, married man!, man! all, all things, things sports!, sports! broadcasting, broadcasting dreams!]                        |\n",
      "|[benefiting refugees, refugees &, & immigrants, immigrants with, with damn, damn good, good entertainment, entertainment ||, || april, april 24,, 24, 2017, 2017 ||, || currently, currently accepting, accepting performer, performer submissions,, submissions, at, at https://t.co/ftq0xuznxo]                                                |\n",
      "|[access hollywood, hollywood /, / creator, creator of, of @wordaful, @wordaful /, / booking, booking inquiries:, inquiries: mecia@rebel-one.net]                                                                                                                                                                                                 |\n",
      "|[producer/songwriter/lead singer, singer of, of sixx:a.m.]                                                                                                                                                                                                                                                                                       |\n",
      "|[ceo @shapeways., @shapeways. i, i love, love 3d, 3d printers,, printers, making, making stuff,, stuff, digital, digital manufacturing,, manufacturing, the, the internet, internet and, and a, a good, good conversation.]                                                                                                                      |\n",
      "|[two division, division ufc, ufc world, world champion., champion. two, two division, division cage, cage warriors, warriors world, world champion., champion. making, making history, history everyday!!, everyday!! #notorious, #notorious #eire]                                                                                              |\n",
      "|[moderator of, of @meetthepress, @meetthepress and, and @nbcnews, @nbcnews political, political director;, director; covering, covering politics, politics since, since '92;, '92; and,, and, yes,, yes, i, i tweet, tweet about, about sports, sports too.]                                                                                     |\n",
      "|[tweeting every, every letter, letter in, in the, the english, english language.]                                                                                                                                                                                                                                                                |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(wordsData)\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inputCol: input column name. (current: words)\\nn: number of elements per n-gram (>=1) (default: 2, current: 2)\\noutputCol: output column name. (default: NGram_2cf15b34fc22__output, current: ngrams)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
