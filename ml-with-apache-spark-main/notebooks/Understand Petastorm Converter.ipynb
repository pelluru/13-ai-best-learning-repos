{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "005dd9db",
   "metadata": {},
   "source": [
    "# Understanding Petastorm converter and types ( Chapter 7 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b24399fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9267c5",
   "metadata": {},
   "source": [
    "Define schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30d625d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/petastorm/spark/spark_dataset_converter.py:28: FutureWarning: pyarrow.LocalFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  from pyarrow import LocalFileSystem\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\n",
    "from petastorm.etl.dataset_metadata import materialize_dataset\n",
    "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n",
    "\n",
    "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
    "from petastorm import TransformSpec \n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "from pyspark.sql.types import (ArrayType, BinaryType, BooleanType, ByteType,\n",
    "                               DoubleType, FloatType, IntegerType, LongType,\n",
    "                               ShortType, StringType, StructField, StructType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b220d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark session:\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Create petastorm store\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\",True) \\\n",
    "    .config(\"spark.memory.offHeap.size\",\"30g\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, 'file:/home/jovyan/petastorm_tmp_cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600f77ff",
   "metadata": {},
   "source": [
    "## understand primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2dd05a",
   "metadata": {},
   "outputs": [],
   "source": [
    " schema = StructType([\n",
    "        StructField(\"bool_col\", BooleanType(), False),\n",
    "        StructField(\"float_col\", FloatType(), False),\n",
    "        StructField(\"double_col\", DoubleType(), False),\n",
    "        StructField(\"short_col\", ShortType(), False),\n",
    "        StructField(\"int_col\", IntegerType(), False),\n",
    "        StructField(\"long_col\", LongType(), False),\n",
    "        StructField(\"str_col\", StringType(), False),\n",
    "        StructField(\"bin_col\", BinaryType(), False),\n",
    "        StructField(\"byte_col\", ByteType(), False),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "881bd1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "        [(True, 0.12, 432.1, 5, 5, 0, \"hello\",\n",
    "          bytearray(b\"spark\\x01\\x02\"), -128),\n",
    "         \n",
    "         (False, 123.45, 0.987, 9, 908, 765, \"petastorm\",\n",
    "          bytearray(b\"\\x0012345\"), 127)],\n",
    "        schema=schema).coalesce(1)\n",
    "\n",
    "    # If we use numPartition > 1 in coalesce, the order of the loaded dataset would\n",
    "    # be non-deterministic.\n",
    "# just for the learning phase - DO NOT USE IN PRODUCTION!    \n",
    "expected_df = df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8067f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(bool_col=True, float_col=0.11999999731779099, double_col=432.1, short_col=5, int_col=5, long_col=0, str_col='hello', bin_col=bytearray(b'spark\\x01\\x02'), byte_col=-128),\n",
       " Row(bool_col=False, float_col=123.44999694824219, double_col=0.987, short_col=9, int_col=908, long_col=765, str_col='petastorm', bin_col=bytearray(b'\\x0012345'), byte_col=127)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ae907",
   "metadata": {},
   "source": [
    " ## Test TensorFlow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ce600f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf operations graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2338ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  self._filesystem = pyarrow.localfs\n",
      "Converting floating-point columns to float32\n",
      "/opt/conda/lib/python3.9/site-packages/petastorm/tf_utils.py:133: UnicodeWarning: Tensorflow will convert all unicode strings back to bytes type. You may need to decode values.\n",
      "  warnings.warn(\"Tensorflow will convert all unicode strings back to bytes type. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferred_schema_view(bool_col=TensorSpec(shape=(None,), dtype=tf.bool, name=None), float_col=TensorSpec(shape=(None,), dtype=tf.float32, name=None), double_col=TensorSpec(shape=(None,), dtype=tf.float32, name=None), short_col=TensorSpec(shape=(None,), dtype=tf.int16, name=None), int_col=TensorSpec(shape=(None,), dtype=tf.int32, name=None), long_col=TensorSpec(shape=(None,), dtype=tf.int64, name=None), str_col=TensorSpec(shape=(None,), dtype=tf.string, name=None), bin_col=TensorSpec(shape=(None,), dtype=tf.string, name=None), byte_col=TensorSpec(shape=(None,), dtype=tf.int8, name=None))\n",
      "inferred_schema_view(bool_col=<tf.Tensor: shape=(32,), dtype=bool, numpy=\n",
      "array([ True, False,  True, False,  True, False,  True, False,  True,\n",
      "       False,  True, False,  True, False,  True, False,  True, False,\n",
      "        True, False,  True, False,  True, False,  True, False,  True,\n",
      "       False,  True, False,  True, False])>, float_col=<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([1.2000e-01, 1.2345e+02, 1.2000e-01, 1.2345e+02, 1.2000e-01,\n",
      "       1.2345e+02, 1.2000e-01, 1.2345e+02, 1.2000e-01, 1.2345e+02,\n",
      "       1.2000e-01, 1.2345e+02, 1.2000e-01, 1.2345e+02, 1.2000e-01,\n",
      "       1.2345e+02, 1.2000e-01, 1.2345e+02, 1.2000e-01, 1.2345e+02,\n",
      "       1.2000e-01, 1.2345e+02, 1.2000e-01, 1.2345e+02, 1.2000e-01,\n",
      "       1.2345e+02, 1.2000e-01, 1.2345e+02, 1.2000e-01, 1.2345e+02,\n",
      "       1.2000e-01, 1.2345e+02], dtype=float32)>, double_col=<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([432.1  ,   0.987, 432.1  ,   0.987, 432.1  ,   0.987, 432.1  ,\n",
      "         0.987, 432.1  ,   0.987, 432.1  ,   0.987, 432.1  ,   0.987,\n",
      "       432.1  ,   0.987, 432.1  ,   0.987, 432.1  ,   0.987, 432.1  ,\n",
      "         0.987, 432.1  ,   0.987, 432.1  ,   0.987, 432.1  ,   0.987,\n",
      "       432.1  ,   0.987, 432.1  ,   0.987], dtype=float32)>, short_col=<tf.Tensor: shape=(32,), dtype=int16, numpy=\n",
      "array([5, 9, 5, 9, 5, 9, 5, 9, 5, 9, 5, 9, 5, 9, 5, 9, 5, 9, 5, 9, 5, 9,\n",
      "       5, 9, 5, 9, 5, 9, 5, 9, 5, 9], dtype=int16)>, int_col=<tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([  5, 908,   5, 908,   5, 908,   5, 908,   5, 908,   5, 908,   5,\n",
      "       908,   5, 908,   5, 908,   5, 908,   5, 908,   5, 908,   5, 908,\n",
      "         5, 908,   5, 908,   5, 908], dtype=int32)>, long_col=<tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
      "array([  0, 765,   0, 765,   0, 765,   0, 765,   0, 765,   0, 765,   0,\n",
      "       765,   0, 765,   0, 765,   0, 765,   0, 765,   0, 765,   0, 765,\n",
      "         0, 765,   0, 765,   0, 765])>, str_col=<tf.Tensor: shape=(32,), dtype=string, numpy=\n",
      "array([b'hello', b'petastorm', b'hello', b'petastorm', b'hello',\n",
      "       b'petastorm', b'hello', b'petastorm', b'hello', b'petastorm',\n",
      "       b'hello', b'petastorm', b'hello', b'petastorm', b'hello',\n",
      "       b'petastorm', b'hello', b'petastorm', b'hello', b'petastorm',\n",
      "       b'hello', b'petastorm', b'hello', b'petastorm', b'hello',\n",
      "       b'petastorm', b'hello', b'petastorm', b'hello', b'petastorm',\n",
      "       b'hello', b'petastorm'], dtype=object)>, bin_col=<tf.Tensor: shape=(32,), dtype=string, numpy=\n",
      "array([b'spark\\x01\\x02', b'\\x0012345', b'spark\\x01\\x02', b'\\x0012345',\n",
      "       b'spark\\x01\\x02', b'\\x0012345', b'spark\\x01\\x02', b'\\x0012345',\n",
      "       b'spark\\x01\\x02', b'\\x0012345', b'spark\\x01\\x02', b'\\x0012345',\n",
      "       b'spark\\x01\\x02', b'\\x0012345', b'spark\\x01\\x02', b'\\x0012345',\n",
      "       b'spark\\x01\\x02', b'\\x0012345', b'spark\\x01\\x02', b'\\x0012345',\n",
      "       b'spark\\x01\\x02', b'\\x0012345', b'spark\\x01\\x02', b'\\x0012345',\n",
      "       b'spark\\x01\\x02', b'\\x0012345', b'spark\\x01\\x02', b'\\x0012345',\n",
      "       b'spark\\x01\\x02', b'\\x0012345', b'spark\\x01\\x02', b'\\x0012345'],\n",
      "      dtype=object)>, byte_col=<tf.Tensor: shape=(32,), dtype=int8, numpy=\n",
      "array([-128,  127, -128,  127, -128,  127, -128,  127, -128,  127, -128,\n",
      "        127, -128,  127, -128,  127, -128,  127, -128,  127, -128,  127,\n",
      "       -128,  127, -128,  127, -128,  127, -128,  127, -128,  127],\n",
      "      dtype=int8)>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "converter = make_spark_converter(df)\n",
    "with converter.make_tf_dataset() as dataset:\n",
    "        iterator = iter(dataset)\n",
    "        print(iterator.element_spec)\n",
    "        \n",
    "        tensor = iterator.get_next()\n",
    "        print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7bec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
